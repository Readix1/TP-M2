Dans ce ReadMe, je vais essayer d'expliquer ce que contient ce repository.
Ce document m'est aussi destiné pour retrouver du travail qui pourrait me servir.  

Vous pouvez voir tous mes Travaux Pratiques(TP) de ma matière AMAL de M2 à Sorbonne Université. 
AMAL est une matière de DeepLearning essentiellement. Pour tous les TP, une partie du code a été donné. 
Pour les premiers il s'agissait de compléter quelques lignes, pour les derniers seul le chargement des données était fourni.

Voici le sujet de chaque TP: 

TP1 - Les bases des NN : MSE et Lineair forward et backward.

TP2 - Graphe de fonction, autograd optimiseur et module

TP3 - Dataset, Dataloader, GPU, checkpoint et premier réseau : autoencodeur ( highway ? )

TP4 - RNN, classification et génération de séquence

TP5 - LSTM, GRU et Beach-search

TP6 - Traduction et pré-traitement

TP7 - Régularisation L1/L2, Dropout, BatchNorm, LayerNorm, Data augmentation et Lightning

TP8 - Détection de sentiments avec CNN

TP9 - Attention global et simple

TP10 - Self-attention, transformers et positional Embeddings


#########################################################################################################

In this ReadMe, I will try to explain what this repository contains.
This document is also intended for me to find work that could be useful to me.

You can see all my Practical Work (TP) of my M2 AMAL subject at Sorbonne University.
AMAL is essentially a DeepLearning subject. For all the TP,  part of the code has been given.
For the first it was a question of completing a few lines, for the last only the loading of the data was provided.

Here is the subject of each TP:

TP1 - The basics of NN: MSE and Linear forward and backward.

TP2 - Function graph, autograd optimizer and module

TP3 - Dataset, Dataloader, GPU, checkpoint and first network: autoencoder (highway?)

TP4 - RNN, classification and sequence generation

TP5 - LSTM, GRU and Beach-search

TP6 - Translation and pre-processing

TP7 - L1/L2 regularization, Dropout, BatchNorm, LayerNorm, Data augmentation and Lightning

TP8 - Sentiment detection with CNN

TP9 - Global and simple attention

TP10 - Self-Attention, Transformers and Positional Embeddings
